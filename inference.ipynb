{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34090d75-5982-498a-9e66-1d41d6f5cc2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T16:00:59.836365Z",
     "iopub.status.busy": "2023-03-27T16:00:59.836034Z",
     "iopub.status.idle": "2023-03-27T16:03:55.724734Z",
     "shell.execute_reply": "2023-03-27T16:03:55.723218Z",
     "shell.execute_reply.started": "2023-03-27T16:00:59.836338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision 204737fcc5213be86dc9b83f6db42bc7aefb574b) to /tmp/pip-install-cy1sm_07/transformers_b50677dc0652409bbc3a1011775870ac\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-cy1sm_07/transformers_b50677dc0652409bbc3a1011775870ac\n",
      "  Running command git rev-parse -q --verify 'sha^204737fcc5213be86dc9b83f6db42bc7aefb574b'\n",
      "  Running command git fetch -q https://github.com/huggingface/transformers.git 204737fcc5213be86dc9b83f6db42bc7aefb574b\n",
      "  Running command git checkout -q 204737fcc5213be86dc9b83f6db42bc7aefb574b\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit 204737fcc5213be86dc9b83f6db42bc7aefb574b\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate===0.18.0\n",
      "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes==0.37.2\n",
      "  Downloading bitsandbytes-0.37.2-py3-none-any.whl (84.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tree-sitter==0.2.2\n",
      "  Downloading tree_sitter-0.2.2.tar.gz (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.6/110.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate===0.18.0->-r requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate===0.18.0->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate===0.18.0->-r requirements.txt (line 1)) (1.12.1+cu116)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate===0.18.0->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate===0.18.0->-r requirements.txt (line 1)) (1.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers@ git+https://github.com/huggingface/transformers.git@204737fcc5213be86dc9b83f6db42bc7aefb574b->-r requirements.txt (line 3)) (2.1.1)\n",
      "Building wheels for collected packages: tree-sitter, transformers\n",
      "  Building wheel for tree-sitter (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tree-sitter: filename=tree_sitter-0.2.2-cp39-cp39-linux_x86_64.whl size=401829 sha256=bf2ca0ba1e822caca4de37d473f5686a9b2e75326edea1d0e1f354f81a56f4cf\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/d0/64/5f7a2fb3662007105ff2b126171b56dc5f6a215fa302047f25\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6827728 sha256=d2b08f6763f268e554c5e0d6bc3a830d3b2c3d2e7fd7f48fa44713be96d80550\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/7c/7e/4b87bd3d3afc0449db9066d10bc96cfac063acced0432f99b7\n",
      "Successfully built tree-sitter transformers\n",
      "Installing collected packages: bitsandbytes, tree-sitter, accelerate, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "Successfully installed accelerate-0.18.0 bitsandbytes-0.37.2 transformers-4.28.0.dev0 tree-sitter-0.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers[deepspeed] in /usr/local/lib/python3.9/dist-packages (4.28.0.dev0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (1.23.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (0.12.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (23.0)\n",
      "Collecting deepspeed>=0.8.3\n",
      "  Downloading deepspeed-0.8.3.tar.gz (765 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.4/765.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from transformers[deepspeed]) (0.18.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate>=0.10.0->transformers[deepspeed]) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate>=0.10.0->transformers[deepspeed]) (1.12.1+cu116)\n",
      "Collecting hjson\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ninja\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from deepspeed>=0.8.3->transformers[deepspeed]) (1.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers[deepspeed]) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers[deepspeed]) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers[deepspeed]) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers[deepspeed]) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers[deepspeed]) (2.8)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.8.3-py3-none-any.whl size=776396 sha256=c7cf3087af64a037838e395ee263047bc09f53363cbfb11ec82ab97fd9405865\n",
      "  Stored in directory: /root/.cache/pip/wheels/7b/ac/1c/f2433006afc1e8f7b2d379220c038ffa0c3f442f8ac4109575\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, ninja, hjson, deepspeed\n",
      "Successfully installed deepspeed-0.8.3 hjson-3.1.0 ninja-1.11.1 py-cpuinfo-9.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting mpi4py\n",
      "  Downloading mpi4py-3.1.4.tar.gz (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: mpi4py\n",
      "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpi4py: filename=mpi4py-3.1.4-cp39-cp39-linux_x86_64.whl size=3380590 sha256=06c5bcd7b4c38ebedaae8ede1470275ffa7441ba2969fbbcb0c19685a2d9c342\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/23/8b/d9d6121a48c86c64bb52f8ffe195f0569caa30ca7179530452\n",
      "Successfully built mpi4py\n",
      "Installing collected packages: mpi4py\n",
      "Successfully installed mpi4py-3.1.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install transformers[deepspeed]\n",
    "!pip install mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96323f2a-1e27-4df6-b7a5-2a48c49d7f4a",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0ac5a0-f226-408b-bbac-91df21eff233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T16:07:52.947182Z",
     "iopub.status.busy": "2023-03-27T16:07:52.946755Z",
     "iopub.status.idle": "2023-03-27T16:07:54.110444Z",
     "shell.execute_reply": "2023-03-27T16:07:54.109810Z",
     "shell.execute_reply.started": "2023-03-27T16:07:52.947154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Problem</th>\n",
       "      <th>Python Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Write a NumPy program to repeat elements of an...</td>\n",
       "      <td>import numpy as np\\rx = np.repeat(3, 4)\\rprint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Write a Python function to create and print a ...</td>\n",
       "      <td>def printValues():\\n\\tl = list()\\n\\tfor i in r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Write a Python program to remove duplicates fr...</td>\n",
       "      <td>import itertools\\rnum = [[10, 20], [40], [30, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Write a NumPy program to compute the x and y c...</td>\n",
       "      <td>import numpy as np\\rimport matplotlib.pyplot a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Write a Python program to alter a given SQLite...</td>\n",
       "      <td>import sqlite3\\rfrom sqlite3 import Error\\rdef...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            Problem  \\\n",
       "0           0  Write a NumPy program to repeat elements of an...   \n",
       "1           1  Write a Python function to create and print a ...   \n",
       "2           2  Write a Python program to remove duplicates fr...   \n",
       "3           3  Write a NumPy program to compute the x and y c...   \n",
       "4           4  Write a Python program to alter a given SQLite...   \n",
       "\n",
       "                                         Python Code  \n",
       "0  import numpy as np\\rx = np.repeat(3, 4)\\rprint...  \n",
       "1  def printValues():\\n\\tl = list()\\n\\tfor i in r...  \n",
       "2  import itertools\\rnum = [[10, 20], [40], [30, ...  \n",
       "3  import numpy as np\\rimport matplotlib.pyplot a...  \n",
       "4  import sqlite3\\rfrom sqlite3 import Error\\rdef...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "raw_data = pd.read_csv(\"ProblemSolutionPythonV3.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25eef18a-1b31-4e02-ba0c-0276a710706d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T16:07:57.631979Z",
     "iopub.status.busy": "2023-03-27T16:07:57.630800Z",
     "iopub.status.idle": "2023-03-27T16:07:57.906222Z",
     "shell.execute_reply": "2023-03-27T16:07:57.904930Z",
     "shell.execute_reply.started": "2023-03-27T16:07:57.631934Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.dropna()\n",
    "data_path = \"CodeT5/data/concode/\"\n",
    "data = []\n",
    "for _, line in raw_data.iterrows():\n",
    "  data.append({\n",
    "      \"code\": line[\"Python Code\"], \n",
    "      \"nl\": line[\"Problem\"]\n",
    "  })\n",
    "\n",
    "train_size = int(len(data) * 0.9)\n",
    "with open(os.path.join(data_path, \"train.json\"), \"w\") as fp:\n",
    "  for ind in range(0, train_size):\n",
    "    fp.write(json.dumps(data[ind]))\n",
    "    fp.write(\"\\n\")\n",
    "\n",
    "with open(os.path.join(data_path, \"dev.json\"), \"w\") as fp:\n",
    "  for ind in range(train_size, len(data)):\n",
    "    fp.write(json.dumps(data[ind]))\n",
    "    fp.write(\"\\n\")\n",
    "\n",
    "with open(os.path.join(data_path, \"test.json\"), \"w\") as fp:\n",
    "  for ind in range(train_size, len(data)):\n",
    "    fp.write(json.dumps(data[ind]))\n",
    "    fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cefcd-3a6b-4ca8-98c9-27657c7f9b93",
   "metadata": {},
   "source": [
    "Hyperparameters chosen for fine-tuning:\n",
    "- batch size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74a75db2-3560-4b61-9ef2-329495989bee",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-27T16:08:01.624362Z",
     "iopub.status.busy": "2023-03-27T16:08:01.623744Z",
     "iopub.status.idle": "2023-03-27T16:11:38.219233Z",
     "shell.execute_reply": "2023-03-27T16:11:38.217814Z",
     "shell.execute_reply.started": "2023-03-27T16:08:01.624334Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================Start Running==========================\n",
      "bash CodeT5/sh/exp_with_args.sh concode none codet5_base 0 -1 8 10 320 150 3 30 1000 saved_models tensorboard results/concode_codet5_base.txt true ./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin None\n",
      "\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n",
      "03/27/2023 16:08:24 - INFO - __main__ -   Namespace(task='concode', sub_task='none', lang='java', eval_task='', model_type='codet5', add_lang_ids=False, data_num=-1, start_epoch=0, num_train_epochs=30, patience=3, cache_path='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/cache_data', summary_dir='tensorboard', data_dir='CodeT5/data', res_dir='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/prediction', res_fn='results/concode_codet5_base.txt', add_task_prefix=False, save_last_checkpoints=True, always_save_model=True, do_eval_bleu=True, compress='None', model_name_or_path='Salesforce/codet5-base', output_dir='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30', load_model_path='./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin', train_filename=None, dev_filename=None, test_filename=None, config_name='', tokenizer_name='Salesforce/codet5-base', max_source_length=320, max_target_length=150, do_train=True, do_eval=True, do_test=True, do_lower_case=False, no_cuda=False, train_batch_size=8, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=0.0001, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, save_steps=-1, log_steps=-1, max_steps=-1, eval_steps=-1, train_steps=-1, warmup_steps=1000, local_rank=-1, seed=1234)\n",
      "03/27/2023 16:08:24 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 8\n",
      "Downloading (…)lve/main/config.json: 100%|██| 1.57k/1.57k [00:00<00:00, 126kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 703k/703k [00:00<00:00, 6.81MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 294k/294k [00:00<00:00, 6.62MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|█████| 2.00/2.00 [00:00<00:00, 488B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|█| 12.5k/12.5k [00:00<00:00, 3.45MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██| 1.48k/1.48k [00:00<00:00, 361kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|███| 892M/892M [00:10<00:00, 88.0MB/s]\n",
      "03/27/2023 16:08:41 - INFO - models -   Finish loading model [223M] from Salesforce/codet5-base\n",
      "03/27/2023 16:08:41 - INFO - models -   Reload model from ./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin\n",
      "03/27/2023 16:08:57 - INFO - utils -   Read 2975 examples, avg src len: 14, avg trg len: 56, max src len: 107, max trg len: 740\n",
      "03/27/2023 16:08:57 - INFO - utils -   [TOKENIZE] avg src len: 17, avg trg len: 199, max src len: 192, max trg len: 5901\n",
      "03/27/2023 16:08:57 - INFO - utils -   Load cache data from saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/cache_data/train_all.pt\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "03/27/2023 16:08:57 - INFO - __main__ -   ***** Running training *****\n",
      "03/27/2023 16:08:57 - INFO - __main__ -     Num examples = 2975\n",
      "03/27/2023 16:08:57 - INFO - __main__ -     Batch size = 8\n",
      "03/27/2023 16:08:57 - INFO - __main__ -     Batch num = 372\n",
      "03/27/2023 16:08:57 - INFO - __main__ -     Num epoch = 30\n",
      "[0] Train loss 0.073:  61%|███████████▌       | 227/372 [02:40<01:40,  1.44it/s]^C\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.9/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# checkpoint is saved at ./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin\n",
    "!python CodeT5/sh/run_exp.py --task concode --sub_task none --model_tag codet5_base --do_train true --checkpoint_path ./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62585f95-ee07-409e-b44b-c66ddfd28c70",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f30133-ffb0-4ffb-8160-0644062dfad8",
   "metadata": {},
   "source": [
    "Instructions: load two checkpoints from here https://drive.google.com/drive/folders/1Z9WmhoMdgk-_DRY5QMhVhX_EtMJH2RvU?usp=share_link\n",
    "\n",
    "Create folder to unzip checkpoints there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf0ae8-2c97-46b2-a175-4ab6601d8c78",
   "metadata": {},
   "source": [
    "Run inference for the initial CodeT5 model for java. Codebleu = 12.8152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b40ea4b-04dc-4e87-a18d-52f90de3f2f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T20:07:21.746490Z",
     "iopub.status.busy": "2023-03-27T20:07:21.745998Z",
     "iopub.status.idle": "2023-03-27T20:37:22.010236Z",
     "shell.execute_reply": "2023-03-27T20:37:22.008882Z",
     "shell.execute_reply.started": "2023-03-27T20:07:21.746490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================Start Running==========================\n",
      "bash CodeT5/sh/exp_with_args.sh concode none codet5_base 0 -1 8 10 320 150 3 30 1000 saved_models tensorboard results/concode_codet5_base.txt false None None\n",
      "\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n",
      "03/27/2023 20:07:36 - INFO - __main__ -   Namespace(task='concode', sub_task='none', lang='java', eval_task='', model_type='codet5', add_lang_ids=False, data_num=-1, start_epoch=0, num_train_epochs=30, patience=3, cache_path='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/cache_data', summary_dir='tensorboard', data_dir='CodeT5/data', res_dir='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/prediction', res_fn='results/concode_codet5_base.txt', add_task_prefix=False, save_last_checkpoints=True, always_save_model=True, do_eval_bleu=True, compress='None', model_name_or_path='Salesforce/codet5-base', output_dir='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30', load_model_path='None', train_filename=None, dev_filename=None, test_filename=None, config_name='', tokenizer_name='Salesforce/codet5-base', max_source_length=320, max_target_length=150, do_train=False, do_eval=True, do_test=True, do_lower_case=False, no_cuda=False, train_batch_size=8, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=0.0001, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, save_steps=-1, log_steps=-1, max_steps=-1, eval_steps=-1, train_steps=-1, warmup_steps=1000, local_rank=-1, seed=1234)\n",
      "03/27/2023 20:07:36 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 8\n",
      "03/27/2023 20:07:41 - INFO - models -   Finish loading model [223M] from Salesforce/codet5-base\n",
      "03/27/2023 20:07:44 - INFO - __main__ -     ***** Testing *****\n",
      "03/27/2023 20:07:44 - INFO - __main__ -     Batch size = 8\n",
      "03/27/2023 20:07:44 - INFO - utils -   Read 2000 examples, avg src len: 68, avg trg len: 29, max src len: 333, max trg len: 123\n",
      "03/27/2023 20:07:44 - INFO - utils -   Create cache data into saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/cache_data/test_src_all.pt\n",
      "100%|██████████████████████████████████████| 2000/2000 [00:02<00:00, 934.15it/s]\n",
      "03/27/2023 20:07:47 - INFO - __main__ -     ***** Running bleu evaluation on test data*****\n",
      "03/27/2023 20:07:47 - INFO - __main__ -     Num examples = 2000\n",
      "03/27/2023 20:07:47 - INFO - __main__ -     Batch size = 8\n",
      "Eval bleu for test set: 100%|█████████████████| 250/250 [27:58<00:00,  6.71s/it]\n",
      "WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "ngram match: 0.00022355292955679812, weighted ngram match: 0.00028074255917998666, syntax_match: 0.5121056493030081, dataflow_match: 0\n",
      "03/27/2023 20:37:19 - INFO - __main__ -   ***** Eval results *****\n",
      "03/27/2023 20:37:19 - INFO - __main__ -     bleu = 0.04\n",
      "03/27/2023 20:37:19 - INFO - __main__ -     codebleu = 12.8152\n",
      "03/27/2023 20:37:19 - INFO - __main__ -     em = 0.0\n",
      "03/27/2023 20:37:19 - INFO - __main__ -   [best-bleu] bleu-4: 0.04, em: 0.0000, codebleu: 12.8152\n",
      "\n",
      "03/27/2023 20:37:19 - INFO - __main__ -   Finish and take 29m\n"
     ]
    }
   ],
   "source": [
    "!python CodeT5/sh/run_exp.py --task concode --sub_task none \\\n",
    "    --model_tag codet5_base --do_train false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9b0625-bbbf-4520-ac0f-d5414a5d4a3d",
   "metadata": {},
   "source": [
    "Run inference for fine-tuned model. Pass path of the checkpoint here. Resu;ts should be bleu = 20.68 and codebleu = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4da2857d-6e94-4c8b-af32-c1b2881c26a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T20:52:05.689528Z",
     "iopub.status.busy": "2023-03-27T20:52:05.688759Z",
     "iopub.status.idle": "2023-03-27T20:59:52.287368Z",
     "shell.execute_reply": "2023-03-27T20:59:52.285899Z",
     "shell.execute_reply.started": "2023-03-27T20:52:05.689498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================Start Running==========================\n",
      "bash CodeT5/sh/exp_with_args.sh concode_nl2py none codet5_base 0 -1 8 10 320 150 3 30 1000 saved_models tensorboard results/concode_nl2py_codet5_base.txt false ./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin None\n",
      "\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n",
      "03/27/2023 20:52:17 - INFO - __main__ -   Namespace(task='concode_nl2py', sub_task='none', lang='', eval_task='', model_type='codet5', add_lang_ids=False, data_num=-1, start_epoch=0, num_train_epochs=30, patience=3, cache_path='saved_models/concode_nl2py/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/cache_data', summary_dir='tensorboard', data_dir='CodeT5/data', res_dir='saved_models/concode_nl2py/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/prediction', res_fn='results/concode_nl2py_codet5_base.txt', add_task_prefix=False, save_last_checkpoints=True, always_save_model=True, do_eval_bleu=True, compress='None', model_name_or_path='Salesforce/codet5-base', output_dir='saved_models/concode_nl2py/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30', load_model_path='./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin', train_filename=None, dev_filename=None, test_filename=None, config_name='', tokenizer_name='Salesforce/codet5-base', max_source_length=320, max_target_length=150, do_train=False, do_eval=True, do_test=True, do_lower_case=False, no_cuda=False, train_batch_size=8, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=0.0001, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, save_steps=-1, log_steps=-1, max_steps=-1, eval_steps=-1, train_steps=-1, warmup_steps=1000, local_rank=-1, seed=1234)\n",
      "03/27/2023 20:52:17 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 8\n",
      "03/27/2023 20:52:21 - INFO - models -   Finish loading model [223M] from Salesforce/codet5-base\n",
      "03/27/2023 20:52:21 - INFO - models -   Reload model from ./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin\n",
      "03/27/2023 20:52:25 - INFO - __main__ -     ***** Testing *****\n",
      "03/27/2023 20:52:25 - INFO - __main__ -     Batch size = 8\n",
      "03/27/2023 20:52:25 - INFO - __main__ -   Reload model from saved_models/concode_nl2py/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin\n",
      "03/27/2023 20:52:27 - INFO - utils -   Read 331 examples, avg src len: 11, avg trg len: 56, max src len: 59, max trg len: 618\n",
      "03/27/2023 20:52:27 - INFO - utils -   Create cache data into saved_models/concode_nl2py/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/cache_data/test_src_all.pt\n",
      "100%|████████████████████████████████████████| 331/331 [00:01<00:00, 318.67it/s]\n",
      "03/27/2023 20:52:28 - INFO - __main__ -     ***** Running bleu evaluation on test data*****\n",
      "03/27/2023 20:52:28 - INFO - __main__ -     Num examples = 331\n",
      "03/27/2023 20:52:28 - INFO - __main__ -     Batch size = 8\n",
      "Eval bleu for test set: 100%|███████████████████| 42/42 [07:01<00:00, 10.03s/it]\n",
      "03/27/2023 20:59:50 - INFO - __main__ -   ***** Eval results *****\n",
      "03/27/2023 20:59:50 - INFO - __main__ -     bleu = 20.68\n",
      "03/27/2023 20:59:50 - INFO - __main__ -     em = 3.0211\n",
      "03/27/2023 20:59:50 - INFO - __main__ -   [best-bleu] bleu-4: 20.68, em: 3.0211, codebleu: 0.0000\n",
      "\n",
      "03/27/2023 20:59:50 - INFO - __main__ -   Finish and take 7m\n"
     ]
    }
   ],
   "source": [
    "!python CodeT5/sh/run_exp.py --task concode_nl2py --sub_task none \\\n",
    "    --model_tag codet5_base --do_train false \\\n",
    "    --checkpoint_path ./saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/pytorch_model.bin "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c72e8f9-46b8-4380-9c82-9e4fb113ac6a",
   "metadata": {},
   "source": [
    "## 8-bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818407c-93f8-4e1b-a96b-1b7ce2d3084c",
   "metadata": {},
   "source": [
    "Now test zero degradation matrix multiplication for Large Language Models described here: https://huggingface.co/blog/hf-bitsandbytes-integration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4478a8-462e-4bb7-bca4-de4bba310682",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T21:00:32.923401Z",
     "iopub.status.busy": "2023-03-27T21:00:32.922459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================Start Running==========================\n",
      "bash CodeT5/sh/exp_with_args.sh concode none codet5_base 0 -1 8 10 320 150 3 30 1000 saved_models tensorboard results/concode_codet5_base.txt false None 8bit\n",
      "\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.6/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 116\n",
      "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n",
      "03/27/2023 21:00:47 - INFO - __main__ -   Namespace(task='concode', sub_task='none', lang='java', eval_task='', model_type='codet5', add_lang_ids=False, data_num=-1, start_epoch=0, num_train_epochs=30, patience=3, cache_path='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/cache_data', summary_dir='tensorboard', data_dir='CodeT5/data', res_dir='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/prediction', res_fn='results/concode_codet5_base.txt', add_task_prefix=False, save_last_checkpoints=True, always_save_model=True, do_eval_bleu=True, compress='8bit', model_name_or_path='Salesforce/codet5-base', output_dir='saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30', load_model_path='None', train_filename=None, dev_filename=None, test_filename=None, config_name='', tokenizer_name='Salesforce/codet5-base', max_source_length=320, max_target_length=150, do_train=False, do_eval=True, do_test=True, do_lower_case=False, no_cuda=False, train_batch_size=8, eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=0.0001, beam_size=10, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, save_steps=-1, log_steps=-1, max_steps=-1, eval_steps=-1, train_steps=-1, warmup_steps=1000, local_rank=-1, seed=1234)\n",
      "03/27/2023 21:00:47 - WARNING - configs -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, cpu count: 8\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "Memory footprint: 417452544\n",
      "03/27/2023 21:00:53 - INFO - models -   Finish loading model [81M] from Salesforce/codet5-base\n",
      "03/27/2023 21:00:53 - INFO - __main__ -     ***** Testing *****\n",
      "03/27/2023 21:00:53 - INFO - __main__ -     Batch size = 8\n",
      "03/27/2023 21:00:53 - INFO - utils -   Read 2000 examples, avg src len: 68, avg trg len: 29, max src len: 333, max trg len: 123\n",
      "03/27/2023 21:00:53 - INFO - utils -   Load cache data from saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/cache_data/test_src_all.pt\n",
      "03/27/2023 21:00:53 - INFO - __main__ -     ***** Running bleu evaluation on test data*****\n",
      "03/27/2023 21:00:53 - INFO - __main__ -     Num examples = 2000\n",
      "03/27/2023 21:00:53 - INFO - __main__ -     Batch size = 8\n",
      "Eval bleu for test set:  34%|██████▏           | 86/250 [18:47<34:33, 12.65s/it]"
     ]
    }
   ],
   "source": [
    "!python CodeT5/sh/run_exp.py --task concode --sub_task none \\\n",
    "    --model_tag codet5_base --do_train false \\\n",
    "    --compress 8bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7f244a-0537-4d68-a72e-4a4a3bfb573d",
   "metadata": {},
   "source": [
    "## Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1526c67-c7b6-4361-a03b-8a197cff8cb8",
   "metadata": {},
   "source": [
    "Do not run 2 cells below. It was used to train a student network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce266987-4575-4ba6-a952-3a1d080f525d",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-27T17:16:57.726553Z",
     "iopub.status.busy": "2023-03-27T17:16:57.726214Z",
     "iopub.status.idle": "2023-03-27T17:17:16.149099Z",
     "shell.execute_reply": "2023-03-27T17:17:16.146991Z",
     "shell.execute_reply.started": "2023-03-27T17:16:57.726525Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def restructure_data(src_dir, task):\n",
    "  dst_dir = os.path.join(src_dir, f\"{task}_distillation\")\n",
    "  os.mkdir(dst_dir)\n",
    "\n",
    "  for file in ['train', 'dev', 'test']:\n",
    "    out_file = file if file != 'dev' else 'val'\n",
    "    with open(os.path.join(src_dir, task, file + \".json\")) as fp1, open(os.path.join(dst_dir, out_file + \".source\"), 'w') as fp2, open(os.path.join(dst_dir, out_file + \".target\"), 'w') as fp3:\n",
    "      for line in fp1:\n",
    "        data = json.loads(line)\n",
    "        fp2.write(data['nl'] + '\\n')\n",
    "        fp3.write(data['code'] + '\\n')\n",
    "\n",
    "/notebooks/saved_models/concode/codet5_base_all_lr10_bs8_src320_trg150_pat3_e30/checkpoint-best-bleu/\n",
    "restructure_data('./CodeT5/data', 'concode')\n",
    "\n",
    "!python distillation.py \\\n",
    "  --teacher Salesforce/codet5-base --data_dir data/concode_distillation \\\n",
    "  --tokenizer_name Salesforce/codet5-base \\\n",
    "  --student_decoder_layers 6 --student_encoder_layers 12 \\\n",
    "  --freeze_encoder --freeze_embeds \\\n",
    "  --learning_rate=3e-4 \\\n",
    "  --do_train \\\n",
    "  --do_predict \\\n",
    "  --fp16 --fp16_opt_level=O1 \\\n",
    "  --val_check_interval 0.1 --n_val 1000 --eval_beams 2 --length_penalty=0.5 \\\n",
    "  --max_target_length=60 --val_max_target_length=60 --test_max_target_length=100 \\\n",
    "  --model_name_or_path IGNORED \\\n",
    "  --alpha_hid=3. \\\n",
    "  --train_batch_size=8 --eval_batch_size=8 --gradient_accumulation_steps=2 \\\n",
    "  --sortish_sampler \\\n",
    "  --num_train_epochs=6 \\\n",
    "  --warmup_steps 500 \\\n",
    "  --output_dir distilt5_12_6 \\\n",
    "  --task translation \\\n",
    "  --overwrite_output_dir \\\n",
    "  \"$@\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13be5f3-b7da-45e4-b7af-5acd2bba66cf",
   "metadata": {},
   "source": [
    "Now insert path to student model here and run cell to see example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c82cbe2-90fb-4907-8246-5c22bdc37de5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-03-27T19:03:27.506571Z",
     "iopub.status.busy": "2023-03-27T19:03:27.506110Z",
     "iopub.status.idle": "2023-03-27T19:03:32.194607Z",
     "shell.execute_reply": "2023-03-27T19:03:32.193224Z",
     "shell.execute_reply.started": "2023-03-27T19:03:27.506527Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/notebooks/transformers/examples/research_projects/seq2seq-distillation/distillation.py\", line 12, in <module>\n",
      "    from finetune import SummarizationModule, TranslationModule\n",
      "  File \"/notebooks/transformers/examples/research_projects/seq2seq-distillation/finetune.py\", line 16, in <module>\n",
      "    from callbacks import Seq2SeqLoggingCallback, get_checkpoint_callback, get_early_stopping_callback\n",
      "  File \"/notebooks/transformers/examples/research_projects/seq2seq-distillation/callbacks.py\", line 10, in <module>\n",
      "    from utils import save_json\n",
      "  File \"/notebooks/transformers/examples/research_projects/seq2seq-distillation/utils.py\", line 17, in <module>\n",
      "    from sacrebleu import corpus_bleu\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sacrebleu/__init__.py\", line 21, in <module>\n",
      "    from .utils import smart_open, SACREBLEU_DIR, download_test_set  # noqa: F401\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sacrebleu/utils.py\", line 597, in <module>\n",
      "    from .dataset import DATASETS, SUBSETS, DOMAINS, COUNTRIES\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sacrebleu/dataset/__init__.py\", line 73, in <module>\n",
      "    from .wmt_xml import WMTXMLDataset\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/sacrebleu/dataset/wmt_xml.py\", line 3, in <module>\n",
      "    import lxml.etree as ET\n",
      "ModuleNotFoundError: No module named 'lxml.etree'\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "# specify path to student model here\n",
    "PATH_TO_STUDENT = '/content/LMInference/transformers/examples/research_projects/seq2seq-distillation/distilt5_12_6/student'\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "model_student = T5ForConditionalGeneration.from_pretrained(PATH_TO_STUDENT)\n",
    "\n",
    "text = \"def greet(user): print(f'hello <extra_id_0>!')\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# simply generate one code span\n",
    "generated_ids = model.generate(input_ids)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "generated_ids_student = model_student.generate(input_ids)\n",
    "print(tokenizer.decode(generated_ids_student[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b1a7f0-58a6-4b90-a9bb-96d1aa7c8946",
   "metadata": {},
   "source": [
    "Run next cell to compare memory occupied by models. Results should be 891528192 for main model and 664980480 for student model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed7de6-0f8f-410d-a727-6b693cc6f9a0",
   "metadata": {},
   "source": [
    "Test BLEU for initial model is 8.2807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60f31b-85cc-4b62-98af-2816cb7075f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint())\n",
    "print(model_student.get_memory_footprint())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
